# Opensearch Cluster

## How to increase the ES index limit

### Sample issue which you see in graylog logs

```raw
[194]: index [graylog_2], type [_doc], id [938e8b50-2d32-11ec-b77c-0ad5e9494873], message [ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=Limit of total fields [1000] has been exceeded]]]
```

## Solution

The issue is related to elasticseach settings and those are stored internally (in a settings index/db) and
can only be adjusted via ES API request.

* Login to the opensearch `opensearch-cluster-master-0` pod

* Verify the existing setting of the index for which it is complaining. You can do so by running

```sh
curl -u $username:$password -XGET http://localhost:9200/$index_name/_settings?pretty=true
```

* Increase the limit by running

```sh
curl -u $username:$password -X PUT "http://localhost:9200/$index_num/_settings?pretty" -H 'Content-Type: application/json' -d'
 {
   "index" : {
     "mapping.total_fields.limit" : 2000
   }
 }'
```

NOTE: We should notice that this increases memory usage and differing field names SHOULD be avoided when possible.
But with graylog, fields are generated by graylog - so not something we can do much about.

## Setting users and passwords

get a shell inside opensearch master pod, and run:

```bash
cd plugins/opensearch-security/tools
chmod +x hash.sh
./hash.sh NEWPASSWORD
# use the generate hash from above - when modifying internal_users.yml to suit your needs
vi /usr/share/opensearch/config/opensearch-security/internal_users.yml
../securityadmin.sh -cd ../securityconfig/ -icl -nhnv -cacert ../../../config/root-ca.pem -cert ../../../config/kirk.pem -key ../../../config/kirk-key.pem
```

## Snapshot opensearch to s3

Create a secret with template functionality

* Copy the example files into a kubeaid-config/k8s/$cluster-name/sealed-secret/graylog/s3-backup.yaml

* Create the secret

```bash
# kubectl create secret generic s3-backup -n graylog --dry-run=client --from-literal=username=admin --from-literal=password=xxxx -o yaml | kubeseal --controller-namespace system --controller-name sealed-secrets -o yaml --merge-into kubeaid-config/k8s/$cluster-name/sealed-secret/graylog/s3-backup.yaml
```

## Down sizing the cluster

### Down sizing might take hours, so its not a 30 min job. so relax and enjoy :)

* Stop routing any shards to the node you want.

  NOTE: Always take the last node in the cluster. if you are downsizing.
  f.exp, if you cluster has 12 pods and start with 12th pod and put the IP in the curl command given below.

```sh
# curl -v -H 'Content-type: application/json' -XPUT 'http://admin:lolpassword@opensearch-cluster-master:9200/_cluster/settings' -d '{
  "transient" :{
     "cluster.routing.allocation.exclude._ip" : "<last-pod-in-the-cluster-ip>"
   }
}'
```

* Verify if all the shards are moved

```bash
# curl -s http://admin:lolpassword@opensearch-cluster-master:9200/_cat/shards | grep <last-pod-name-in-the-cluster>

# `relocating_shards` should be **0**

# curl -s http://admin:lolpassworl@opensearch-cluster-master:9200/_cluster/health | jq
{
  "cluster_name": "opensearch-cluster",
  "status": "green",
  "timed_out": false,
  "number_of_nodes": 12,
  "number_of_data_nodes": 12,
  "discovered_master": true,
  "active_primary_shards": 3963,
  "active_shards": 3979,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 100
}
```

* Set the replicas count to n - 1 (12 pods in the cluster - 1 == 11)
  NOTE: Do this only when cluster is green

```yaml
opensearch:
  replicas: 11
```

* Sync it on the argocd and it will restart the whole cluster and remove the last pod in the cluster.

## Upgrade Instruction

* !! TAKE A SNAPSHOT FIRST !! (opensearch s3 snapshot, this should be configured as part of installation)

  ```bash
  # NOTE: this might take sometime depending on the size of the cluster, around 4TB takes about 60m (so watch out)
  kubectl create job -n graylog --from=cronjob/backup-s3 opensearch-manual-backup-01

  or you can get snapshot via api (Havent tried this)
  ```

* Verify which version of opensearch works with graylog
* opensearch cluster are not downgradable, so please restore it from snapshot (look at opensearch helm chart readme)

## Restore Instruction

* Delete the statefulset

```bash
kubectl delete sts opensearch-cluster-master -n graylog --cascade=orphan
```

* Delete the pod one by one (opensearch pod)
* change the PV to `RETAIN` and change the claimRef to diff name, so new cluster build does not take the same pv.
  I have did it directly from the k9s console. But be careful with this.

```bash
# Do this for all the opensearch PV's
kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
```

* Once the pv is set to `RETAIN` and now we can delete the PVC
* Deploy the new cluster in the same namespace
* Add s3 repo

```bash
curl -s -u admin:admin -X PUT http://opensearch-cluster-master:9200/_snapshot/ops-s3 -d '{"type": "s3", "settings": { "bucket": "<bucket-name>" } }'
```

* List snapshots

```bash
curl -s -u admin:admin -X GET http://opensearch-cluster-master:9200/_snapshot/ops-s3/_all?pretty
```

* Restore from the snapshot

```bash
curl -u admin:admin -X POST "opensearch-cluster-master:9200/_snapshot/ops-s3/<snapshot-name>/_restore?pretty" -H 'Content-Type: application/json'  -d '{
  "indices": "-.opendistro_security",
  "include_global_state": false
}'
```
