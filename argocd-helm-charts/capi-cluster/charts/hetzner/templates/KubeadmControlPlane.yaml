apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: {{ .Values.global.clusterName }}-control-plane
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          authorization-mode: Node,RBAC
          client-ca-file: /etc/kubernetes/pki/ca.crt
          cloud-provider: external
          default-not-ready-toleration-seconds: "45"
          default-unreachable-toleration-seconds: "45"
          enable-aggregator-routing: "true"
          enable-bootstrap-token-auth: "true"
          etcd-cafile: /etc/kubernetes/pki/etcd/ca.crt
          etcd-certfile: /etc/kubernetes/pki/etcd/server.crt
          etcd-keyfile: /etc/kubernetes/pki/etcd/server.key
          kubelet-client-certificate: /etc/kubernetes/pki/apiserver-kubelet-client.crt
          kubelet-client-key: /etc/kubernetes/pki/apiserver-kubelet-client.key
          kubelet-preferred-address-types: ExternalIP,Hostname,InternalDNS,ExternalDNS
          profiling: "false"
          proxy-client-cert-file: /etc/kubernetes/pki/front-proxy-client.crt
          proxy-client-key-file: /etc/kubernetes/pki/front-proxy-client.key
          requestheader-allowed-names: front-proxy-client
          requestheader-client-ca-file: /etc/kubernetes/pki/front-proxy-ca.crt
          requestheader-extra-headers-prefix: X-Remote-Extra-
          requestheader-group-headers: X-Remote-Group
          requestheader-username-headers: X-Remote-User
          service-account-key-file: /etc/kubernetes/pki/sa.pub
          service-account-lookup: "true"
          tls-cert-file: /etc/kubernetes/pki/apiserver.crt
          tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
          tls-private-key-file: /etc/kubernetes/pki/apiserver.key
      controllerManager:
        extraArgs:
          allocate-node-cidrs: "true"
          authentication-kubeconfig: /etc/kubernetes/controller-manager.conf
          authorization-kubeconfig: /etc/kubernetes/controller-manager.conf
          bind-address: 0.0.0.0
          cloud-provider: external
          cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt
          cluster-signing-duration: 6h0m0s
          cluster-signing-key-file: /etc/kubernetes/pki/ca.key
          kubeconfig: /etc/kubernetes/controller-manager.conf
          profiling: "false"
          requestheader-client-ca-file: /etc/kubernetes/pki/front-proxy-ca.crt
          root-ca-file: /etc/kubernetes/pki/ca.crt
          secure-port: "10257"
          service-account-private-key-file: /etc/kubernetes/pki/sa.key
          terminated-pod-gc-threshold: "10"
          use-service-account-credentials: "true"
      etcd:
        local:
          dataDir: /var/lib/etcd
          extraArgs:
            auto-tls: "false"
            cert-file: /etc/kubernetes/pki/etcd/server.crt
            cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
            client-cert-auth: "true"
            key-file: /etc/kubernetes/pki/etcd/server.key
            peer-auto-tls: "false"
            peer-client-cert-auth: "true"
            trusted-ca-file: /etc/kubernetes/pki/etcd/ca.crt
      scheduler:
        extraArgs:
          bind-address: 0.0.0.0
          kubeconfig: /etc/kubernetes/scheduler.conf
          profiling: "false"
          secure-port: "10259"
    files:
    {{- if eq .Values.mode "bare-metal" }}
    - content: |
        [Unit]
        Description=Cilium BPF mounts
        Documentation=https://docs.cilium.io/
        DefaultDependencies=no
        Before=local-fs.target umount.target
        After=swap.target

        [Mount]
        What=bpffs
        Where=/sys/fs/bpf
        Type=bpf
        Options=rw,nosuid,nodev,noexec,relatime,mode=700

        [Install]
        WantedBy=multi-user.target
      owner: "root:root"
      path: /etc/systemd/system/sys-fs-bpf.mount
      permissions: "0744"
    {{- end }}
    - content: |
        net.ipv4.conf.lxc*.rp_filter = 0
      owner: root:root
      path: /etc/sysctl.d/99-cilium.conf
      permissions: "0744"
    - content: |
        overlay
        br_netfilter
      owner: root:root
      path: /etc/modules-load.d/crio.conf
      permissions: "0744"
    - content: |
        net.bridge.bridge-nf-call-iptables  = 1
        net.bridge.bridge-nf-call-ip6tables = 1
        net.ipv4.ip_forward                 = 1
      owner: root:root
      path: /etc/sysctl.d/99-kubernetes-cri.conf
      permissions: "0744"
    - content: |
        vm.overcommit_memory=1
        kernel.panic=10
        kernel.panic_on_oops=1
      owner: root:root
      path: /etc/sysctl.d/99-kubelet.conf
      permissions: "0744"
    - content: |
        nameserver 1.1.1.1
        nameserver 1.0.0.1
        nameserver 2606:4700:4700::1111
      owner: root:root
      path: /etc/kubernetes/resolv.conf
      permissions: "0744"
    - content: |
        # Copyright The containerd Authors.
        #
        # Licensed under the Apache License, Version 2.0 (the "License");
        # you may not use this file except in compliance with the License.
        # You may obtain a copy of the License at
        #
        #     http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #
        # https://raw.githubusercontent.com/containerd/containerd/main/containerd.service

        [Unit]
        Description=containerd container runtime
        Documentation=https://containerd.io
        After=network.target local-fs.target dbus.service

        [Service]
        ExecStartPre=-/sbin/modprobe overlay
        ExecStart=/usr/local/bin/containerd

        Type=notify
        Delegate=yes
        KillMode=process
        Restart=always
        RestartSec=5

        # Having non-zero Limit*s causes performance problems due to accounting overhead
        # in the kernel. We recommend using cgroups to do container-local accounting.
        LimitNPROC=infinity
        LimitCORE=infinity

        # Comment TasksMax if your systemd version does not supports it.
        # Only systemd 226 and above support this version.
        TasksMax=infinity
        OOMScoreAdjust=-999

        [Install]
        WantedBy=multi-user.target
      owner: root:root
      path: /etc/systemd/system/containerd.service
      permissions: "0744"
    {{/*
      TODO : We need to apply the netplan config, only when using a Floating IP.
             Otherwise, when using a single master node for the control-plane, we just provide
             that master node's IP.
    */}}
    {{- if eq .Values.mode "bare-metal" }}
    {{/*
      Netplan is the default network configuration utility on Ubuntu. It configures network
      interfaces using YAML files, which are then translated into systemd-networkd or
      NetworkManager configurations.

      Using Netplan, we'll bind the Floating IP with the master node's primary network interface.
    */}}
    - content: |
        #!/bin/bash
        wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
        chmod a+x /usr/local/bin/yq

        interface_name=$(/usr/local/bin/yq '.network.ethernets | keys | .[0]' /etc/netplan/01-netcfg.yaml)

        touch /etc/netplan/60-floating-ip.yaml && chmod 600 /etc/netplan/60-floating-ip.yaml
        cat <<EOT > /etc/netplan/60-floating-ip.yaml
        network:
          version: 2
          renderer: networkd
          ethernets:
            $interface_name:
              addresses:
                - {{ .Values.controlPlane.bareMetal.endpoint.host }}/32
        EOT

        netplan apply
      owner: root:root
      path: /bind-floating-ip.sh
      permissions: "0744"
    {{- end }}
    initConfiguration:
      skipPhases:
        - addon/kube-proxy
      nodeRegistration:
        kubeletExtraArgs:
          anonymous-auth: "false"
          authentication-token-webhook: "true"
          authorization-mode: Webhook
          cloud-provider: external
          event-qps: "5"
          kubeconfig: /etc/kubernetes/kubelet.conf
          max-pods: "120"
          read-only-port: "0"
          resolv-conf: /etc/kubernetes/resolv.conf
          rotate-server-certificates: "true"
          tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          anonymous-auth: "false"
          authentication-token-webhook: "true"
          authorization-mode: Webhook
          cloud-provider: external
          event-qps: "5"
          kubeconfig: /etc/kubernetes/kubelet.conf
          max-pods: "120"
          read-only-port: "0"
          resolv-conf: /etc/kubernetes/resolv.conf
          rotate-server-certificates: "true"
          tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
    {{- if (gt (len $.Values.global.additionalUsers) 0) }}
    users:
      {{- range $index, $additionalUser := $.Values.global.additionalUsers }}
      - name: {{ $additionalUser.name }}
        sshAuthorizedKeys:
          - {{ $additionalUser.sshPublicKey }}
      {{- end }}
    {{- end }}
    preKubeadmCommands:
      - set -x
      - export CONTAINERD=2.1.3
      - export RUNC=1.2.5
      - export KUBERNETES_VERSION=$(echo {{ $.Values.global.kubernetes.version }} | sed 's/^v//')
      - export TRIMMED_KUBERNETES_VERSION=$(echo ${KUBERNETES_VERSION} | sed 's/^v//' | awk -F . '{print $1 "." $2}')
      - ARCH="$(dpkg --print-architecture)"
      - localectl set-locale LANG=en_US.UTF-8
      - localectl set-locale LANGUAGE=en_US.UTF-8
      - apt-get update -y
      - apt-get -y install at jq unzip wget logrotate apt-transport-https

      {{- if eq .Values.mode "bare-metal" }}

      {{/*
        On the first master node (where 'kubeadm init' is executed),
        we need to have this netplan config in effect, before 'kubeadm init' execution. Otherwise,
        preflight checks will fail and 'kubeadm init' won't succeed.

        On the second and other remaining master nodes, after 'kubeadm join' is executed,
        we will create and apply this same netplan config.
        If we have this netplan config before executing 'kubeadm join', then the Floating IP
        will be attached to its own primary network interface as well. Which will cause it to
        loopback to itself, instead of reaching out to the first master node.
      */}}
      - |
        [ ! -f /run/kubeadm/kubeadm-join-config.yaml ] && \
          chmod +x /bind-floating-ip.sh && \
          /bind-floating-ip.sh

      {{/* BUG : The kubeadm-join-config.yaml gets removed before 'kubeadm join' execution. */}}
      - |
        [ -f /run/kubeadm/kubeadm-join-config.yaml ] && \
          cp /run/kubeadm/kubeadm-join-config.yaml /kubeadm-join-config.yaml
      {{- end }}

      - sed -i '/swap/d' /etc/fstab
      - swapoff -a
      - modprobe overlay && modprobe br_netfilter && sysctl --system
      - wget https://github.com/opencontainers/runc/releases/download/v$RUNC/runc.$ARCH
      - wget https://github.com/opencontainers/runc/releases/download/v$RUNC/runc.sha256sum
      - sha256sum --check --ignore-missing runc.sha256sum
      - install runc.$ARCH /usr/local/sbin/runc
      - rm -f runc.$ARCH runc.sha256sum
      - wget https://github.com/containerd/containerd/releases/download/v$CONTAINERD/containerd-$CONTAINERD-linux-$ARCH.tar.gz
      - wget https://github.com/containerd/containerd/releases/download/v$CONTAINERD/containerd-$CONTAINERD-linux-$ARCH.tar.gz.sha256sum
      - sha256sum --check containerd-$CONTAINERD-linux-$ARCH.tar.gz.sha256sum
      - tar -zxf containerd-$CONTAINERD-linux-$ARCH.tar.gz -C /usr/local
      - rm -f containerd-$CONTAINERD-linux-$ARCH.tar.gz containerd-$CONTAINERD-linux-$ARCH.tar.gz.sha256sum
      - mkdir -p /etc/containerd
      - containerd config default > /etc/containerd/config.toml
      - sed -i  "s/SystemdCgroup = false/SystemdCgroup = true/" /etc/containerd/config.toml
      - systemctl daemon-reload && systemctl enable containerd && systemctl start containerd
      - mkdir -p /etc/apt/keyrings/
      - curl -fsSL https://pkgs.k8s.io/core:/stable:/v$TRIMMED_KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      - echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$TRIMMED_KUBERNETES_VERSION/deb/
        /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
      - apt-get update
      - apt-get install -y kubelet="$KUBERNETES_VERSION-*" kubeadm="$KUBERNETES_VERSION-*"
        kubectl="$KUBERNETES_VERSION-*" bash-completion && apt-mark hold kubelet kubectl
        kubeadm && systemctl enable kubelet
      - kubeadm config images pull --kubernetes-version $KUBERNETES_VERSION
      - echo 'source <(kubectl completion bash)' >>/root/.bashrc
      - echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >>/root/.bashrc
      - apt-get -y autoremove && apt-get -y clean all

      {{/* BUG : The kubeadm-join-config.yaml gets removed before 'kubeadm join' execution. */}}
      {{- if eq .Values.mode "bare-metal" }}
      - |
        [ -f /kubeadm-join-config.yaml ] && \
          mkdir -p /var/run/kubeadm && \
          cp /kubeadm-join-config.yaml /run/kubeadm/kubeadm-join-config.yaml
      {{- end }}

    postKubeadmCommands:
      {{/* On the second or remaining master nodes, after 'kubeadm join' execution,
           we create and apply the netplan config. */}}
      {{- if eq .Values.mode "bare-metal" }}
      - |
        [ -f /run/kubeadm/kubeadm-join-config.yaml ] && \
          chmod +x /bind-floating-ip.sh && \
          /bind-floating-ip.sh
      {{- end }}

      # Install Helm.
      - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      # Clone KubeAid.
      - git clone https://github.com/Obmondo/kubeaid.git

      # Copy kubeconfig to ~/.kube/config
      - mkdir -p $HOME/.kube
      - sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      - sudo chown $(id -u):$(id -g) $HOME/.kube/config

      # Install Cilium.
      - kubectl create namespace cilium
      - API_SERVER_URL=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
      - API_SERVER_HOST=$(echo "$API_SERVER_URL" | awk -F[/:] '{print $4}')
      - API_SERVER_PORT=$(echo "$API_SERVER_URL" | awk -F[/:] '{print $5}')

      {{- if or (eq .Values.mode "hcloud") (eq .Values.mode "hybrid") }}
      - |
        helm template kubeaid/argocd-helm-charts/cilium \
          --values kubeaid/argocd-helm-charts/cilium/values.yaml \
          --set cilium.kubeProxyReplacement=true \
          --set cilium.k8sServiceHost=${API_SERVER_HOST} \
          --set cilium.k8sServicePort=${API_SERVER_PORT} \
          --set cilium.extraArgs[0]="--devices=eth0" \
          --namespace cilium | kubectl apply -f -
      {{- else }}
      - |
        helm template kubeaid/argocd-helm-charts/cilium \
          --values kubeaid/argocd-helm-charts/cilium/values.yaml \
          --set cilium.kubeProxyReplacement=true \
          --set cilium.k8sServiceHost=${API_SERVER_HOST} \
          --set cilium.k8sServicePort=${API_SERVER_PORT} \
          --namespace cilium | kubectl apply -f -
      {{- end }}

      {{- if or (eq .Values.mode "hcloud") (eq .Values.mode "hybrid") }}
      # Install HCloud Cloud Controller Manager.
      - |
        helm template ccm-hcloud kubeaid/argocd-helm-charts/ccm-hcloud \
          --values kubeaid/argocd-helm-charts/ccm-hcloud/values.yaml \
          --namespace kube-system | kubectl apply -f -
      {{- end }}

      {{- if or (eq .Values.mode "bare-metal") (eq .Values.mode "hybrid") }}
      # Install Hetzner Bare Metal Cloud Controller Manager.
      - |
        helm template ccm-hetzner kubeaid/argocd-helm-charts/ccm-hetzner \
          --values kubeaid/argocd-helm-charts/ccm-hetzner/values.yaml \
          --namespace kube-system | kubectl apply -f -
      {{- end }}

      - apt-get -y install socat mtr
      - apt-get -y autoremove && apt-get -y clean all
    {{- if and ($.Values.global.additionalUsers) (gt (len $.Values.global.additionalUsers) 0) }}
    users:
      {{- range $index, $additionalUser := $.Values.global.additionalUsers }}
      - name: {{ $additionalUser.name }}
        sshAuthorizedKeys:
          - {{ $additionalUser.sshPublicKey }}
      {{- end }}
    {{- end }}
  {{- if eq .Values.mode "bare-metal" }}
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: HetznerBareMetalMachineTemplate
      name: {{ .Values.global.clusterName }}-control-plane
  replicas: {{ len .Values.controlPlane.bareMetal.bareMetalHosts }}
  {{- else }}
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: HCloudMachineTemplate
      name: {{ .Values.global.clusterName }}-control-plane
  replicas: {{ .Values.controlPlane.hcloud.replicas }}
  {{- end }}
  version: {{ .Values.global.kubernetes.version }}
